{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "nFxgu88ydnJL",
        "outputId": "287f8160-6e71-4a34-ba9c-7663ac06dfe9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'creditcard.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c787b5e9c743>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"creditcard.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# manual parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'creditcard.csv'"
          ]
        }
      ],
      "source": [
        "# read & manipulate data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# visualisations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style='whitegrid', context='notebook')\n",
        "%matplotlib notebook\n",
        "\n",
        "# misc\n",
        "import random as rn\n",
        "\n",
        "# load the dataset\n",
        "df = pd.read_csv(\"creditcard.csv\")\n",
        "\n",
        "# manual parameters\n",
        "RANDOM_SEED = 42\n",
        "TRAINING_SAMPLE = 200000\n",
        "VALIDATE_SIZE = 0.2\n",
        "\n",
        "# setting random seeds for libraries to ensure reproducibility\n",
        "np.random.seed(RANDOM_SEED)\n",
        "rn.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "# let's quickly convert the columns to lower case and rename the Class column\n",
        "# so as to not cause syntax errors\n",
        "df.columns = map(str.lower, df.columns)\n",
        "df.rename(columns={'class': 'label'}, inplace=True)\n",
        "\n",
        "# print first 5 rows to get an initial impression of the data we're dealing with\n",
        "df.head()\n",
        "\n",
        "# add a negligible amount to avoid taking the log of 0\n",
        "df['log10_amount'] = np.log10(df.amount + 0.00001)\n",
        "\n",
        "# manual parameter\n",
        "RATIO_TO_FRAUD = 15\n",
        "\n",
        "# dropping redundant columns\n",
        "df = df.drop(['time', 'amount'], axis=1)\n",
        "\n",
        "# splitting by class\n",
        "fraud = df[df.label == 1]\n",
        "clean = df[df.label == 0]\n",
        "\n",
        "# undersample clean transactions\n",
        "clean_undersampled = clean.sample(\n",
        "    int(len(fraud) * RATIO_TO_FRAUD),\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "# concatenate with fraud transactions into a single dataframe\n",
        "visualisation_initial = pd.concat([fraud, clean_undersampled])\n",
        "column_names = list(visualisation_initial.drop('label', axis=1).columns)\n",
        "\n",
        "# isolate features from labels\n",
        "features, labels = visualisation_initial.drop('label', axis=1).values, \\\n",
        "                   visualisation_initial.label.values\n",
        "print(f\"\"\"The non-fraud dataset has been undersampled from {len(clean):,} to {len(clean_undersampled):,}.\n",
        "This represents a ratio of {RATIO_TO_FRAUD}:1 to fraud.\"\"\")\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def tsne_scatter(features, labels, dimensions=2, save_as='graph.png'):\n",
        "    if dimensions not in (2, 3):\n",
        "        raise ValueError('tsne_scatter can only plot in 2d or 3d (What are you? An alien that can visualise >3d?). Make sure the \"dimensions\" argument is in (2, 3)')\n",
        "\n",
        "    # t-SNE dimensionality reduction\n",
        "    features_embedded = TSNE(n_components=dimensions, random_state=RANDOM_SEED).fit_transform(features)\n",
        "\n",
        "    # initialising the plot\n",
        "    fig, ax = plt.subplots(figsize=(8,8))\n",
        "\n",
        "    # counting dimensions\n",
        "    if dimensions == 3: ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # plotting data\n",
        "    ax.scatter(\n",
        "        *zip(*features_embedded[np.where(labels==1)]),\n",
        "        marker='o',\n",
        "        color='r',\n",
        "        s=2,\n",
        "        alpha=0.7,\n",
        "        label='Fraud'\n",
        "    )\n",
        "    ax.scatter(\n",
        "        *zip(*features_embedded[np.where(labels==0)]),\n",
        "        marker='o',\n",
        "        color='g',\n",
        "        s=2,\n",
        "        alpha=0.3,\n",
        "        label='Clean'\n",
        "    )\n",
        "\n",
        "    # storing it to be displayed later\n",
        "    plt.legend(loc='best')\n",
        "    plt.savefig(save_as);\n",
        "    plt.show;\n",
        "\n",
        "tsne_scatter(features, labels, dimensions=2, save_as='tsne_initial_2d.png')\n",
        "\n",
        "# shuffle our training set\n",
        "clean = clean.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# training set: exlusively non-fraud transactions\n",
        "X_train = clean.iloc[:TRAINING_SAMPLE].drop('label', axis=1)\n",
        "\n",
        "# testing  set: the remaining non-fraud + all the fraud\n",
        "X_test = clean.iloc[TRAINING_SAMPLE:].append(fraud).sample(frac=1)\n",
        "\n",
        "print(f\"\"\"Our testing set is composed as follows:\n",
        "\n",
        "{X_test.label.value_counts()}\"\"\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# train // validate - no labels since they're all clean anyway\n",
        "X_train, X_validate = train_test_split(X_train,\n",
        "                                       test_size=VALIDATE_SIZE,\n",
        "                                       random_state=RANDOM_SEED)\n",
        "\n",
        "# manually splitting the labels from the test df\n",
        "X_test, y_test = X_test.drop('label', axis=1).values, X_test.label.values\n",
        "\n",
        "print(f\"\"\"Shape of the datasets:\n",
        "    training (rows, cols) = {X_train.shape}\n",
        "    validate (rows, cols) = {X_validate.shape}\n",
        "    holdout  (rows, cols) = {X_test.shape}\"\"\")\n",
        "\n",
        "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# configure our pipeline\n",
        "pipeline = Pipeline([('normalizer', Normalizer()),\n",
        "                     ('scaler', MinMaxScaler())])\n",
        "\n",
        "# get normalization parameters by fitting to the training data\n",
        "pipeline.fit(X_train);\n",
        "\n",
        "# transform the training and validation data with these parameters\n",
        "X_train_transformed = pipeline.transform(X_train)\n",
        "X_validate_transformed = pipeline.transform(X_validate)\n",
        "\n",
        "# Load the extension and start TensorBoard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs\n",
        "\n",
        "# data dimensions // hyperparameters\n",
        "input_dim = X_train_transformed.shape[1]\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 100\n",
        "\n",
        "# https://keras.io/layers/core/\n",
        "autoencoder = tf.keras.models.Sequential([\n",
        "\n",
        "    # deconstruct / encode\n",
        "    tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
        "    tf.keras.layers.Dense(16, activation='elu'),\n",
        "    tf.keras.layers.Dense(8, activation='elu'),\n",
        "    tf.keras.layers.Dense(4, activation='elu'),\n",
        "    tf.keras.layers.Dense(2, activation='elu'),\n",
        "\n",
        "    # reconstruction / decode\n",
        "    tf.keras.layers.Dense(4, activation='elu'),\n",
        "    tf.keras.layers.Dense(8, activation='elu'),\n",
        "    tf.keras.layers.Dense(16, activation='elu'),\n",
        "    tf.keras.layers.Dense(input_dim, activation='elu')\n",
        "\n",
        "])\n",
        "\n",
        "# https://keras.io/api/models/model_training_apis/\n",
        "autoencoder.compile(optimizer=\"adam\",\n",
        "                    loss=\"mse\",\n",
        "                    metrics=[\"acc\"])\n",
        "\n",
        "# print an overview of our model\n",
        "autoencoder.summary();\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# current date and time\n",
        "yyyymmddHHMM = datetime.now().strftime('%Y%m%d%H%M')\n",
        "\n",
        "# new folder for a new run\n",
        "log_subdir = f'{yyyymmddHHMM}_batch{BATCH_SIZE}_layers{len(autoencoder.layers)}'\n",
        "\n",
        "# define our early stopping\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0.0001,\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    mode='min',\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "save_model = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='autoencoder_best_weights.hdf5',\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    verbose=0,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "tensorboard = tf.keras.callbacks.TensorBoard(\n",
        "    f'logs/{log_subdir}',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    update_freq='batch'\n",
        ")\n",
        "\n",
        "# callbacks argument only takes a list\n",
        "cb = [early_stop, save_model, tensorboard]\n",
        "\n",
        "history = autoencoder.fit(\n",
        "    X_train_transformed, X_train_transformed,\n",
        "    shuffle=True,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=cb,\n",
        "    validation_data=(X_validate_transformed, X_validate_transformed)\n",
        ");\n",
        "\n",
        "# transform the test set with the pipeline fitted to the training set\n",
        "X_test_transformed = pipeline.transform(X_test)\n",
        "\n",
        "# pass the transformed test set through the autoencoder to get the reconstructed result\n",
        "reconstructions = autoencoder.predict(X_test_transformed)\n",
        "\n",
        "# calculating the mean squared error reconstruction loss per row in the numpy array\n",
        "mse = np.mean(np.power(X_test_transformed - reconstructions, 2), axis=1)\n",
        "\n",
        "clean = mse[y_test==0]\n",
        "fraud = mse[y_test==1]\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "ax.hist(clean, bins=50, density=True, label=\"clean\", alpha=.6, color=\"green\")\n",
        "ax.hist(fraud, bins=50, density=True, label=\"fraud\", alpha=.6, color=\"red\")\n",
        "plt.title(\"(Normalized) Distribution of the Reconstruction Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "THRESHOLD = 3\n",
        "def mad_score(points):\n",
        "    \"\"\"https://www.itl.nist.gov/div898/handbook/eda/section3/eda35h.htm \"\"\"\n",
        "    m = np.median(points)\n",
        "    ad = np.abs(points - m)\n",
        "    mad = np.median(ad)\n",
        "\n",
        "    return 0.6745 * ad / mad\n",
        "z_scores = mad_score(mse)\n",
        "outliers = z_scores > THRESHOLD\n",
        "\n",
        "from sklearn.metrics import (confusion_matrix,\n",
        "                             precision_recall_curve)\n",
        "# get (mis)classification\n",
        "cm = confusion_matrix(y_test, outliers)\n",
        "# true/false positives/negatives\n",
        "(tn, fp,\n",
        " fn, tp) = cm.flatten()\n",
        "\n",
        "print(f\"\"\"The classifications using the MAD method with threshold={THRESHOLD} are as follows:\n",
        "{cm}\n",
        "% of transactions labeled as fraud that were correct (precision): {tp}/({fp}+{tp}) = {tp/(fp+tp):.2%}\n",
        "% of fraudulent transactions were caught succesfully (recall):    {tp}/({fn}+{tp}) = {tp/(fn+tp):.2%}\"\"\")\n",
        "\n",
        "clean = z_scores[y_test==0]\n",
        "fraud = z_scores[y_test==1]\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "ax.hist(clean, bins=50, density=True, label=\"clean\", alpha=.6, color=\"green\")\n",
        "ax.hist(fraud, bins=50, density=True, label=\"fraud\", alpha=.6, color=\"red\")\n",
        "plt.title(\"Distribution of the modified z-scores\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "encoder = tf.keras.models.Sequential(autoencoder.layers[:5])\n",
        "encoder.summary()\n",
        "\n",
        "# taking all the fraud, undersampling clean\n",
        "fraud = X_test_transformed[y_test==1]\n",
        "clean = X_test_transformed[y_test==0][:len(fraud) * RATIO_TO_FRAUD, ]\n",
        "# combining arrays & building labels\n",
        "features = np.append(fraud, clean, axis=0)\n",
        "labels = np.append(np.ones(len(fraud)),\n",
        "                   np.zeros(len(clean)))\n",
        "# getting latent space representation\n",
        "latent_representation = encoder.predict(features)\n",
        "print(f'Clean transactions downsampled from {len(X_test_transformed[y_test==0]):,} to {len(clean):,}.')\n",
        "print('Shape of latent representation:', latent_representation.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read & manipulate data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# visualisations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style='whitegrid', context='notebook')\n",
        "%matplotlib notebook\n",
        "\n",
        "# misc\n",
        "import random as rn\n",
        "\n",
        "# load the dataset\n",
        "df = pd.read_csv(\"creditcard.csv\")\n",
        "\n",
        "# manual parameters\n",
        "RANDOM_SEED = 42\n",
        "TRAINING_SAMPLE = 200000\n",
        "VALIDATE_SIZE = 0.2\n",
        "\n",
        "# setting random seeds for libraries to ensure reproducibility\n",
        "np.random.seed(RANDOM_SEED)\n",
        "rn.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "# let's quickly convert the columns to lower case and rename the Class column\n",
        "# so as to not cause syntax errors\n",
        "df.columns = map(str.lower, df.columns)\n",
        "df.rename(columns={'class': 'label'}, inplace=True)"
      ],
      "metadata": {
        "id": "c2N3jHw3lFZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CtHwpsS1lNpe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}